{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exam for course Algorithms in Bioinformatics\n",
    "Course number #22125/#22175.\n",
    "\n",
    "June 24th 2020 9.00-13.00\n",
    "\n",
    "The exam is open book meaning that you can seek information at the internet. You however cannot consult other students taken the exam, and plagiarism is seriously condemned.\n",
    "\n",
    "Note, that you can compile your exam answers in any text editing program you prefer. You are NOT requested to use Jupyter-notebooks.  \n",
    "\n",
    "The exam must however be submitted as a jupyter-notebook or a single PDF file via the course side at DTU Inside at the June 24th 2020 13.00 CEST.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (20 points)\n",
    "### a)\n",
    "From the sequences below:\n",
    "\n",
    "```python\n",
    "IEK\n",
    "SEK\n",
    "TEK\n",
    "TEA\n",
    "ADA```\n",
    "\n",
    "calculate the weight matrix scores for **K**, and **E** at **position 3** in the binding motif using pseudo counts with weight on prior **$\\beta = 20$** (ignoring sequence weighting). Describe how you arrived at the matrix scores by reporting the values of f (the observed frequency), g (the pseudo frequency), and p (the combined frequency), as well as the final weight matrix value, w, for each amino acid **K** and **E**.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_K= 0.6\n",
      "f_E= 0.0\n",
      "f_A= 0.4\n",
      "g_K= 0.184\n",
      "g_E= 0.058\n",
      "g_A= 0.152\n",
      "p_K= 0.25333333333333335\n",
      "p_E= 0.04833333333333334\n",
      "p_A= 0.19333333333333336\n",
      "W_K= 1.4742631340658479\n",
      "W_E= -0.11086259281180957\n",
      "W_A= 0.9603507216681857\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "alpha = n-1\n",
    "beta = 20\n",
    "\n",
    "## The following is for position 3.\n",
    "f_K = 3/n\n",
    "f_E = 0/n\n",
    "f_A = 2/n\n",
    "print(\"f_K=\",f_K)\n",
    "print(\"f_E=\",f_E)\n",
    "print(\"f_A=\",f_A)\n",
    "\n",
    "g_K = f_K*0.28+f_A*0.04\n",
    "g_E = f_K*0.07+f_A*0.04\n",
    "g_A = f_K*0.06+f_A*0.29\n",
    "print(\"g_K=\",g_K)\n",
    "print(\"g_E=\",g_E)\n",
    "print(\"g_A=\",g_A)\n",
    "\n",
    "p_K = (alpha*f_K + beta*g_K)/(alpha+beta)\n",
    "p_E = (alpha*f_E + beta*g_E)/(alpha+beta)\n",
    "p_A = (alpha*f_A + beta*g_A)/(alpha+beta)\n",
    "\n",
    "print(\"p_K=\",p_K)\n",
    "print(\"p_E=\",p_E)\n",
    "print(\"p_A=\",p_A)\n",
    "\n",
    "W_K = np.log(p_K/0.058)\n",
    "W_E = np.log(p_E/0.054)\n",
    "W_A = np.log(p_A/0.074)\n",
    "\n",
    "print(\"W_K=\",W_K)\n",
    "print(\"W_E=\",W_E)\n",
    "print(\"W_A=\",W_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "If the values in the weight matrix scores for S at positions 1 is 2.1 and the value for D at position 2 is 3.1, what is the total weight matrix score for the two peptides below\n",
    "\n",
    "1. SDK\n",
    "2. SDA\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK: 6.674263134065848\n",
      "SDA: 6.160350721668186\n"
     ]
    }
   ],
   "source": [
    "SDK_score = 2.1 + 3.1 + W_K\n",
    "SDA_score = 2.1 + 3.1 + W_A\n",
    "\n",
    "print(\"SDK:\",SDK_score)\n",
    "print(\"SDA:\",SDA_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Which of the two peptides has the highest likelihood of binding, and why?\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SDK scores the highest and therefore has the highest likelihood of binding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Calculate the weight matrix value for **L** at position **1** from the alignment below containing only one peptide sequence \n",
    "\n",
    "**LEV**\n",
    "\n",
    "Include in some details on how you have arrived at the obtained result.\n",
    "\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Only having 1 sequence means that alpha becomes 0. This means that the weight matrix values for position 1 becomes the L row of the BLOSUM matrix. Therefore the weight for L becomes 0.38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (15 points)\n",
    "### a)\n",
    "Complete the accumulative alignment matrix D and the E matrix below by replacing the X’s with the correct values for the alignment of the two sequences \n",
    "\n",
    "```python\n",
    "query = \"VAAAAP\"\n",
    "database = \"VAAQVAAP\"\n",
    "```\n",
    "\n",
    "aligned using the Blosum50 scoring matrix with gap penalties\n",
    "\n",
    "```python\n",
    "gap_open = -2\n",
    "gap_extension = -1\n",
    "```\n",
    "\n",
    "and the database sequence, as always, is scored along the horizontal direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MATRIX = \"\"\"\n",
    "XX 25 21 20 XX 16 10  4  0\n",
    "25 XX 23 19 18 17 11  5  0\n",
    "19 21 XX XX 20 18 12  6  0\n",
    "14 15 16 17 18 20 13  7  0\n",
    " 8  9 10 11 12 13 15  8  0\n",
    " 2  3  4  5  6  7  8 XX  0\n",
    " 0  0  0  0  0  0  0  0  0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "E_MATRIX = \"\"\"\n",
    "X  2  2  4  X  3  3  3  0\n",
    "4  X  1  1  1  1  1  3  0\n",
    "4  1  X  X  1  1  1  3  0\n",
    "5  1  1  5  4  1  1  3  0\n",
    "5  1  1  5  5  1  1  2  0\n",
    "5  5  5  5  5  5  4  X  0\n",
    "0  0  0  0  0  0  0  0  0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "d_matrix = np.array(D_MATRIX.strip().split()).reshape(7,-1)\n",
    "e_matrix = np.array(E_MATRIX.strip().split()).reshape(7,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In your answer, please include some details on how you have arrived at the obtained results.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Answers were obtained using the O3 algorithm, using BLOSUM50 for the match values.\n",
    "d_matrix[0,0] = 32\n",
    "d_matrix[0,4] = 17\n",
    "d_matrix[1,1] = 27\n",
    "d_matrix[2,2] = 22\n",
    "d_matrix[2,3] = 18\n",
    "d_matrix[5,7] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_matrix[0,0] = 1\n",
    "e_matrix[0,4] = 1\n",
    "e_matrix[1,1] = 1\n",
    "e_matrix[2,2] = 1\n",
    "e_matrix[2,3] = 4\n",
    "e_matrix[5,7] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (5 points)\n",
    "### a) \n",
    "Say you are running a Metropolis Monte Carlo calculation to maximize a given fitness-function. Your original configuration has a fitness of 0.5, and you updated configuration has a fitness of 0.7. What is the probability of accepting the updated configuration at a value of T=0.2? \n",
    "\n",
    "In your answer, please include some details on how you have arrived at the obtained result.\n",
    "\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_accept: 0.3678794411714424\n"
     ]
    }
   ],
   "source": [
    "# Assuming higher fitness is worse (like in MSE):\n",
    "T = 0.2\n",
    "E_1 = 0.7\n",
    "E_0 = 0.5\n",
    "d_E = E_1-E_0\n",
    "P_accept = min(1, np.exp(-d_E/T))\n",
    "\n",
    "print(\"P_accept:\", P_accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Explain briefly the function of the scale T in the Metropolis Monte Carlo calculation \n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "T can be seen as a \"temperature\" that determines how willing the MC algorithm is to accept unfavorable weight changes. This is to combat getting stuck in local minima. Higher T = higher willingness to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 points)\n",
    "Describe briefly in one short sentence for each what the three HMM scoring methods do:\n",
    "\n",
    "- Viterbi\n",
    "- Forward\n",
    "- Posterior decoding\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Vinterbi algorithm findes the most probable way a sequence has come to be.\n",
    "The forward algorithm sums the probaility of all the ways a sequence could come to be working from the beginning of the sequence to the full sequence.\n",
    "The posterior decoding works kinda opposite of the forward algorithm where it asks what the probability of a certain coming from a specific previous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (15 points)\n",
    "The following figure describes an HMM model of an unfair casino playing “head and tails” with a loaded coin (**note, a picture should load below)**:\n",
    "\n",
    "![HMM model](https:www.cbs.dtu.dk/courses/22125/exam_2020/HMM.png)\n",
    "\n",
    "The arrows indicate the different transition probabilities and the values in the square the probabilities of getting head (H) and tail (T) with each of the two coins. When the model is used, a fair or loaded coin is selected at random initially.\n",
    "\n",
    "### a)\n",
    "In the casino, you observed the following outcome HHTTHH after the casino has thrown the coin six times. Use the Viterbi algorithm to fill out the missing parts (XX) of the table below (note that we here use raw and NOT log-transformed probabilities). \n",
    "\n",
    "![Viterbi matrix](https:www.cbs.dtu.dk/courses/22125/exam_2021/Viterbi.png)\n",
    "\n",
    "In your answer, please include some details on how you have arrived at the obtained result.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05358569999999999, 0.003281]\n",
      "[0.0028203, 0.029529]\n",
      "[0.00063635, 0.0021519]\n",
      "\n",
      "HHTT_F: 0.026792849999999997 Most likely from fair coin\n",
      "HHTT_L: 0.0029529 Most likely from loaded coin\n",
      "HHTTHH_L: 0.0019367100000000001 Most likely from loaded coin\n"
     ]
    }
   ],
   "source": [
    "# Using the following algorithm P_l(i+1) = p_l(i+1)*max_k(P_k(i)*a_kl)\n",
    "\n",
    "print([0.95*0.056406, 0.1*0.03281])\n",
    "print([0.05*0.056406, 0.9*0.03281])\n",
    "print([0.05*0.012727, 0.9*0.002391])\n",
    "\n",
    "HHTT_F = 0.5 * np.max([0.95*0.056406, 0.1*0.03281])\n",
    "HHTT_L = 0.1 * np.max([0.05*0.056406, 0.9*0.03281])\n",
    "HHTTHH_L = 0.9 * np.max([0.05*0.012727, 0.9*0.002391])\n",
    "\n",
    "print()\n",
    "print(\"HHTT_F:\",HHTT_F, \"Most likely from fair coin\")\n",
    "print(\"HHTT_L:\",HHTT_L,\"Most likely from loaded coin\")\n",
    "print(\"HHTTHH_L:\",HHTTHH_L,\"Most likely from loaded coin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "What was the most likely list of coins used to make this outcome, i.e. write down which series of coins (F/L) were most likely used to make the outcome.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looking at the highest probabilities across the table, the most likely list of coins is: FFFFFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Explain how the values in the first column (0.25 and 0.45) are calculated\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is unknown whether we startede with a fair or loaded coin, so it is assumed to be 0.5 for each coin.\n",
    "so:\n",
    "0.25 = 0.5 * 0.5\n",
    "0.45 = 0.5 * 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (5 points)\n",
    "Calculate the output from the ANN below using the step-function as activation function with y=0, if x<=0, and y=1 otherwise. **Note, a picture should load below).**\n",
    "\n",
    "![ANN model](https:www.cbs.dtu.dk/courses/22125/exam_2020/ANN.png)\n",
    "\n",
    "when the input values **I1** and **I2** are as given below. In your answer, please include some details on how you have arrived at the obtained result.\n",
    "\n",
    "### a)\n",
    "```python\n",
    "l1 = 0\n",
    "l2 = 0\n",
    "```\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1: 0\n",
      "O2: 0\n",
      "Y1: 0\n"
     ]
    }
   ],
   "source": [
    "l1 = 0\n",
    "l2 = 0\n",
    "\n",
    "o1 = l1*2+l2*2-3.5\n",
    "o2 = l1*3+l2*3-1\n",
    "\n",
    "O1 = int(o1>0)\n",
    "O2 = int(o2>0)\n",
    "print(\"O1:\",O1)\n",
    "print(\"O2:\",O2)\n",
    "\n",
    "y1 = O1*-1+O2*1-0.5\n",
    "Y1 = int(y1>0)\n",
    "\n",
    "print(\"Y1:\",Y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "```python\n",
    "l1 = 1\n",
    "l2 = 0\n",
    "```\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1: 0\n",
      "O2: 1\n",
      "Y1: 1\n"
     ]
    }
   ],
   "source": [
    "l1 = 1\n",
    "l2 = 0\n",
    "\n",
    "o1 = l1*2+l2*2-3.5\n",
    "o2 = l1*3+l2*3-1\n",
    "O1 = int(o1>0)\n",
    "O2 = int(o2>0)\n",
    "print(\"O1:\",O1)\n",
    "print(\"O2:\",O2)\n",
    "\n",
    "y1 = O1*-1+O2*1-0.5\n",
    "Y1 = int(y1>0)\n",
    "\n",
    "print(\"Y1:\",Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a set of 1000 non-redundant data points. Describe in max 200 words how you would set up a model training and performance evaluation scheme for a one hidden layer neural network method trained excluding early stopping but including hyper-parameter optimization of the number of hidden neurons.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For this I would use a 2 layers of 5 fold cross validation(CV). In the outer layer I would split the data in 5 withhold 1/5 of the data for performance evaluation and send the other 4/5 into the inner layer. In the inner layer I would split up the 5 again and use 1/5 as a test set and 4/5 as a training set. I would then train ANN models for different numbers of hidden neurons to find whats optimal using the test set to determine this. This would happen 5 times with the 1/5 being used as a test set changing each time. From this it would be possible to determine which number of hidden neurons was optimal by evaluation the test performance on the concatenated prediction data for each number of hidden neurons over the 5 folds. This optimal number of hidden neurons would then be used to train a model on all 800 datapoints in the inner layer and the 1/5 withheld in the outer layer would be used to evaluate the model. This can then be repeated for the rest of the folds in the outer layer and performance can be calculated on the on the concatenated prediction data for 5 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (10 points)\n",
    "From the alignment\n",
    "\n",
    "```python\n",
    "ALLPFIVSV \n",
    "ALAKAAAAV \n",
    "ALAKAAAAN \n",
    "ALAKAAAAR \n",
    "ALAKAAAAT \n",
    "ALAKAAAAV \n",
    "GMNERPILT \n",
    "GILGFVFTM \n",
    "TLNAWVKVV \n",
    "KLNEPVLLL \n",
    "```\n",
    "\n",
    "calculate the weight of the first peptide **ALLPFIVSV** using heuristic sequence weighting. Note, that the peptide data set is **NOT** identical to the peptides from the lecture slides.\n",
    "\n",
    "In your answer, please include details on how you have arrived at the obtained result.\n",
    "\n",
    "#### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2083333333333335\n"
     ]
    }
   ],
   "source": [
    "# Using the formula W_k = sum(1/(r_p*s_p)) where k is the peptide, r_p is the number of different amino acids on position p \n",
    "# and s_p is the number of occurences of amino acid a on position p \n",
    "\n",
    "print(1/(4*6) + 1/(3*8) + 1/(3*2) + 1/(5*1) + 1/(5*2) + 1/(4*1) + 1/(6*1) + 1/(5*1) + 1/(6*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 (5 points)\n",
    "Describe briefly the two Hobohm algorithms are implemented and what they are used for\n",
    "\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hobohm1 does this by starting from the top of a sorted list and adding sequences to another list. The sequences are only added if they are not similar to the sequences already on the list. If a sequence is similar to the sequences already on the list, it will be discarted.\n",
    "Hobohm2 is slower and works by aligning all sequences against eachother to begin with. Then it creates a matrix of which sequences are similar to eachother. It will then remove sequences from the matrix starting from the sequence that is similar to most other sequences and continuing until the sequences left in the matrix are not similar to eachother.\n",
    "\n",
    "The Hobohm algorithms are used on your dataset before creating your models in order to mitigate data redundancy that might skew your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 (5 points)\n",
    "Describe briefly at least three approaches that can be applied to avoid/minimize overfitting of machine learning methods\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An important approach is cross validation, where the data is split up in n folds. 1 fold is then picked as a evauation set and the rest are used to construct/train the ML models. The part used for evaluation then changes n times until all folds have been used for evaluation. The optimal model that does not overfit is then choosen as the model that has the best predictions combined across the folds.\n",
    "\n",
    "Another method is using a noise suppressing parameter such as the lambda parameter in the SMM model. \n",
    "\n",
    "A third method is early stopping for trained models. Here a test set seperate from the training set is used to evaluate the performance of the model after each round of training. If the test score starts dropping while the training score keeps increasing it is a sign of overfitting and the training is stopped with the model that performed best on the test set being the model used for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mathias Rahbek-Borre, s183447"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

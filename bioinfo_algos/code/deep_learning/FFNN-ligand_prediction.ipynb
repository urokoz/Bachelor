{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network to predict MHC ligands\n",
    "The notebook consists of the following sections:\n",
    "\n",
    "0. Module imports, define functions, set constants\n",
    "1. Load Data\n",
    "2. Build Model\n",
    "3. Select Hyper-paramerters\n",
    "4. Compile Model\n",
    "5. Train Model\n",
    "6. Evaluation\n",
    "\n",
    "## Exercise\n",
    "\n",
    "The exercise is to optimize the model given in this notebook by selecting hyper-parameters that improve performance. First run the notebook as is and take notes of the performance (AUC, MCC). Then start a manual hyper-parameter search by following the instructions below. If your first run results in poor fitting (the model doesn't learn anything during training) do not dispair! Hopefully you will see a rapid improvement when you start testing other hyper-parameters.\n",
    "\n",
    "### Optimizer, learning rate, and mini-batches\n",
    "The [optimizers](https://pytorch.org/docs/stable/optim.html) are different approaches of minimizing a loss function based on gradients. The learning rate determine to which degree we correct the weights. The smaller the learning rate, the smaller corrections we make. This may prolong the training time. To mitigate this, one can train with mini-batches. Instead of feeding your network all of the data before you make updates you can partition the training data into mini-batches and update weigths more frequently. Thus, your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\n",
    "\n",
    "If you experienced trouble in even training then you might benefit from lowering the learning rate to 0.01 or 0.001 or perhaps even smaller.\n",
    "\n",
    "__Optimizers:__\n",
    "1. SGD (+ Momentum)\n",
    "2. Adam\n",
    "3. Try others if you like...\n",
    "\n",
    "__Mini-batch size:__\n",
    "When you have implemented and tested a smaller learning rate try also implementing a mini-batch of size 512 or 128. In order to set the mini-batch size use the variable MINI_BATCH_SIZE and run train_with_minibatches() instead of train().\n",
    "\n",
    "### Number of hidden units\n",
    "Try increasing the number of model parameters (weights), eg. 64, 128, or 512.\n",
    "\n",
    "### Hidden layers\n",
    "Add another layer to the network. To do so you must edit the methods of Net()-class.\n",
    "\n",
    "### Parameter initialization\n",
    "Parameter initialization can be extremely important.\n",
    "PyTorch has a lot of different [initializers](http://pytorch.org/docs/master/nn.html#torch-nn-init) and the most often used initializers are listed below. Try implementing one of them.\n",
    "1. Kaming He\n",
    "2. Xavier Glorot\n",
    "3. Uniform or Normal with small scale (0.1 - 0.01)\n",
    "\n",
    "Bias is nearly always initialized to zero using the [torch.nn.init.constant(tensor, val)](http://pytorch.org/docs/master/nn.html#torch.nn.init.constant)\n",
    "\n",
    "To implement an initialization method you must uncomment #net.apply(init_weights) and to select your favorite method you must modify the init_weights function.\n",
    "\n",
    "### Nonlinearity\n",
    "Non-linearity is what makes neural networks universal predictors. Not everything in our universe is related by linearity and therefore we must implement non-linear activations to cope with that. [The most commonly used nonliearities](http://pytorch.org/docs/master/nn.html#non-linear-activations) are listed below. \n",
    "1. ReLU\n",
    "2. Leaky ReLU\n",
    "3. Sigmoid squash the output [0, 1], and are used if your output is binary (not used in the hidden layers)\n",
    "4. Tanh is similar to sigmoid, but squashes in [-1, 1]. It is rarely used any more.\n",
    "5. Softmax normalizes the the output to 1, and is used as output if you have a classification problem\n",
    "\n",
    "Change the current function to another. To do so, you must modify the forward()-method in the Net()-class. \n",
    "\n",
    "### Early stopping\n",
    "Early stopping stops your training when you have reached the best possible model before overfitting. The method saves the model weights at each epoch while constantly monitoring the development of the validation loss. Once the validation loss starts to increase the method will raise a flag. The method will allow for a number of epochs to pass before stopping. The number of epochs are referred to as patience. If the validation loss decreases below the previous global minima before the patience runs out the flag and patience is reset. If a new global minima is not encountered the training is stopped and the weights from the global minima epoch are loaded and defines the final model. \n",
    "\n",
    "To implement early stopping you must set implement=True in the invoke()-function called within train() or train_with_minibatches().\n",
    "\n",
    "### Regularization (optional)\n",
    "Implement either L2 regularization, [dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers) or [batch normalization](https://pytorch.org/docs/stable/nn.html#normalization-layers).\n",
    "\n",
    "### Mix of peptide lengths\n",
    "Now you have hopefully found an architecture that yields a pretty good performance. But of course it is not that simple... One of the issues that occur when working with real data is that ligands can have lengths of 8, 10, or 11 amino acids. In order to accomodate different lengths you need to pad your sequences, so they still fit into the expected tensor. This, however, may mess with the weights of the anchor positions.\n",
    "\n",
    "Try and include 8-9-10-11mers and take a look at how it affects performance. \n",
    "\n",
    "* set MAX_PEP_SEQ_LEN = 11\n",
    "* set ALLELE = 'A0301'\n",
    "\n",
    "#### Performance evaluation\n",
    "Run the notebook and take a look at how the model performs on data partitioned by peptide length. \n",
    "\n",
    "1. What happens to the performance evaluated on 8-10-11mers (excluding 9mers) compared to performance evaluated only on peptides of length 9?\n",
    "\n",
    "Can you explain why we would prefer a good performance on 8-9-10-11mers over a higher performance on only 9mers?\n",
    "\n",
    "## ... continue exercise with notebook CNN-ligand_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep='\\s+', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_peptide_target(filename):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\s+', usecols=[0,1], names=['peptide','target'])\n",
    "    return df.sort_values(by='target', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_peptides(Xin):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (batch_size, MAX_PEP_SEQ_LEN, n_features)\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(Xin)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    Xout = np.zeros((batch_size, MAX_PEP_SEQ_LEN, n_features), dtype=np.int8)\n",
    "    \n",
    "    for peptide_index, row in Xin.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            Xout[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return Xout, Xin.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(early_stopping, loss, model, implement=False):\n",
    "    if implement == False:\n",
    "        return False\n",
    "    else:\n",
    "        early_stopping(loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PEP_SEQ_LEN = 9 #11\n",
    "BINDER_THRESHOLD = 0.426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLELE = 'A0201' #'A0301'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum_file = \"../data/BLOSUM50\"\n",
    "train_data = \"../data/%s/train_BA\" % ALLELE\n",
    "valid_data = \"../data/%s/valid_BA\" % ALLELE\n",
    "test_data = \"../data/%s/test_BA\" % ALLELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw = load_peptide_target(train_data)\n",
    "valid_raw = load_peptide_target(valid_data)\n",
    "test_raw = load_peptide_target(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peptide_distribution(raw_data, raw_set):\n",
    "    raw_data['peptide_length'] = raw_data.peptide.str.len()\n",
    "    raw_data['target_binary'] = (raw_data.target >= BINDER_THRESHOLD).astype(int)\n",
    "\n",
    "    # Position of bars on x-axis\n",
    "    ind = np.arange(train_raw.peptide.str.len().nunique())\n",
    "    neg = raw_data[raw_data.target_binary == 0].peptide_length.value_counts().sort_index()\n",
    "    pos = raw_data[raw_data.target_binary == 1].peptide_length.value_counts().sort_index()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    width = 0.3  \n",
    "\n",
    "    plt.bar(ind, neg, width, label='Non-binders')\n",
    "    plt.bar(ind + width, pos, width, label='Binders')\n",
    "\n",
    "    plt.xlabel('Peptide lengths')\n",
    "    plt.ylabel('Count of peptides')\n",
    "    plt.title('Distribution of peptide lengths in %s data' %raw_set)\n",
    "    plt.xticks(ind + width / 2, ['%dmer' %i for i in neg.index])\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peptide_distribution(train_raw, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_values(data=[(train_raw, 'Train set'), (valid_raw, 'Validation set'), (test_raw, 'Test set')]):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    for partition, label in data:\n",
    "        x = partition.index\n",
    "        y = partition.target\n",
    "        plt.scatter(x, y, label=label, marker='.')\n",
    "    plt.axhline(y=BINDER_THRESHOLD, color='r', linestyle='--', label='Binder threshold')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.title('Target values')\n",
    "    plt.xlabel('Index of dependent variable')\n",
    "    plt.ylabel('Dependent varible')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_, y_train_ = encode_peptides(train_raw)\n",
    "x_valid_, y_valid_ = encode_peptides(valid_raw)\n",
    "x_test_, y_test_ = encode_peptides(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data dimensions for the train set and validation set (batch_size, MAX_PEP_SEQ_LEN, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_.shape)\n",
    "print(x_valid_.shape)\n",
    "print(x_test_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ = x_train_.reshape(x_train_.shape[0], -1)\n",
    "x_valid_ = x_valid_.reshape(x_valid_.shape[0], -1)\n",
    "x_test_ = x_test_.reshape(x_test_.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_train_.shape[0]\n",
    "n_features = x_train_.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Variable(torch.from_numpy(x_train_.astype('float32')))\n",
    "y_train = Variable(torch.from_numpy(y_train_.astype('float32'))).view(-1, 1)\n",
    "\n",
    "x_valid = Variable(torch.from_numpy(x_valid_.astype('float32')))\n",
    "y_valid = Variable(torch.from_numpy(y_valid_.astype('float32'))).view(-1, 1)\n",
    "\n",
    "x_test = Variable(torch.from_numpy(x_test_.astype('float32')))\n",
    "y_test = Variable(torch.from_numpy(y_test_.astype('float32'))).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_l1):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_l1)\n",
    "        self.fc2 = nn.Linear(n_l1, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/docs/master/nn.init.html\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0) # alternative command: m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3000\n",
    "MINI_BATCH_SIZE = 512\n",
    "N_HIDDEN_NEURONS = 16\n",
    "LEARNING_RATE = 0.1\n",
    "PATIENCE = EPOCHS // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_features, N_HIDDEN_NEURONS)\n",
    "#net.apply(init_weights)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No mini-batch loading\n",
    "# mini-batch loading\n",
    "def train():\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        net.train()\n",
    "        pred = net(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data)\n",
    "\n",
    "        if epoch % (EPOCHS//10) == 0:\n",
    "            print('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.data))\n",
    "\n",
    "        net.eval()\n",
    "        pred = net(x_valid)\n",
    "        loss = criterion(pred, y_valid)  \n",
    "        valid_loss.append(loss.data)\n",
    "\n",
    "        if invoke(early_stopping, valid_loss[-1], net, implement=True):\n",
    "            net.load_state_dict(torch.load('checkpoint.pt'))\n",
    "            break\n",
    "            \n",
    "    return net, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with mini_batches\n",
    "train_loader = DataLoader(dataset=TensorDataset(x_train, y_train), batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=TensorDataset(x_valid, y_valid), batch_size=MINI_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def train_with_minibatches():\n",
    "    \n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        batch_loss = 0\n",
    "        net.train()\n",
    "        for x_train, y_train in train_loader:\n",
    "            pred = net(x_train)\n",
    "            loss = criterion(pred, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss += loss.data\n",
    "        train_loss.append(batch_loss / len(train_loader))\n",
    "\n",
    "        batch_loss = 0\n",
    "        net.eval()\n",
    "        for x_valid, y_valid in valid_loader:\n",
    "            pred = net(x_valid)\n",
    "            loss = criterion(pred, y_valid)\n",
    "            batch_loss += loss.data\n",
    "        valid_loss.append(batch_loss / len(valid_loader))\n",
    "        \n",
    "        if epoch % (EPOCHS//10) == 0:\n",
    "            print('Train Epoch: {}\\tLoss: {:.6f}\\tVal Loss: {:.6f}'.format(epoch, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "        if invoke(early_stopping, valid_loss[-1], net, implement=True):\n",
    "            net.load_state_dict(torch.load('checkpoint.pt'))\n",
    "            break\n",
    "            \n",
    "    return net, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, train_loss, valid_loss = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net, train_loss, valid_loss = train_with_minibatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(burn_in=20):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.plot(list(range(burn_in, len(train_loss))), train_loss[burn_in:], label='Training loss')\n",
    "    plt.plot(list(range(burn_in, len(valid_loss))), valid_loss[burn_in:], label='Validation loss')\n",
    "\n",
    "    # find position of lowest validation loss\n",
    "    minposs = valid_loss.index(min(valid_loss))+1 \n",
    "    plt.axvline(minposs, linestyle='--', color='r',label='Minimum Validation Loss')\n",
    "\n",
    "    plt.legend(frameon=False)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "pred = net(x_test)\n",
    "loss = criterion(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_values(data=[(pd.DataFrame(pred.data.numpy(), columns=['target']), 'Prediction'),\n",
    "                         (test_raw, 'Target')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform targets to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_class = np.where(y_test.flatten() >= BINDER_THRESHOLD, 1, 0)\n",
    "y_pred_class = np.where(pred.flatten() >= BINDER_THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Caracteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(peptide_length=[9]):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = 'AUC = %0.2f (%smer)' %(roc_auc, '-'.join([str(i) for i in peptide_length])))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1], c='black', linestyle='--')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining targets and prediction values with peptide length in a dataframe\n",
    "pred_per_len = pd.DataFrame([test_raw.peptide.str.len().to_list(),\n",
    "                             y_test_class,\n",
    "                             pred.flatten().detach().numpy()],\n",
    "                            index=['peptide_length','target','prediction']).T\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "# For each peptide length compute AUC and plot ROC\n",
    "for length, grp in pred_per_len.groupby('peptide_length'):\n",
    "    fpr, tpr, threshold = roc_curve(grp.target, grp.prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plot_roc_curve(peptide_length=[int(length)])\n",
    "\n",
    "# Evaluating model on peptides of length other than 9 AA.\n",
    "for lengths in [[8,10,11],[8,9,10,11]]:\n",
    "    grp = pred_per_len[pred_per_len.peptide_length.isin(lengths)]\n",
    "    if not grp.empty:\n",
    "        fpr, tpr, threshold = roc_curve(grp.target, grp.prediction)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plot_roc_curve(peptide_length=lengths)\n",
    "\n",
    "    else:\n",
    "        print(\"Data does not contain peptides of length other than 9 AA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matthew's Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mcc():\n",
    "    plt.title('Matthews Correlation Coefficient')\n",
    "    plt.scatter(y_test.flatten().detach().numpy(), pred.flatten().detach().numpy(), label = 'MCC = %0.2f' % mcc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Validation targets')\n",
    "    plt.show()\n",
    "\n",
    "plot_mcc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

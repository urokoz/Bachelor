# -*- coding: utf-8 -*-
"""Correlation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EI9LpecrRCWa46R_lCSZyxMmxAEOu6nI
"""

#!/usr/bin/env python

import numpy as np
import matplotlib.pyplot as plt
import math
import scipy.stats as st
import pandas as pd
from scipy.stats import spearmanr


#alphabet_file = alphabet_upload.values()
#alphabet_file = "https://raw.githubusercontent.com/brunoalvarez89/data/master/algorithms_in_bioinformatics/part_3/alphabet"
alphabet_file = "Matrices/alphabet"
alphabet = np.loadtxt(alphabet_file, dtype=str)


#blosum_file = "https://raw.githubusercontent.com/brunoalvarez89/data/master/algorithms_in_bioinformatics/part_3/blosum50"
blosum_file = "Matrices/BLOSUM50"
_blosum50 = np.loadtxt(blosum_file, dtype=float).reshape((24, -1)).T

blosum50 = {}

for i, letter_1 in enumerate(alphabet):

    blosum50[letter_1] = {}

    for j, letter_2 in enumerate(alphabet):

        blosum50[letter_1][letter_2] = _blosum50[i, j]


def smith_waterman_alignment(query="VLLP", database="VLILP", scoring_scheme={}, gap_open=-5, gap_extension=-1):

    # Matrix imensions
    M = len(query)
    N = len(database)

    # D matrix change to float
    D_matrix = np.zeros((M+1, N+1), int)

    # P matrix
    P_matrix = np.zeros((M+1, N+1), int)

    # Q matrix
    Q_matrix = np.zeros((M+1, N+1), int)

    # E matrix
    E_matrix = np.zeros((M+1, N+1), dtype=object)

    # Initialize matrices
    for i in range(M, 0, -1):
        # Here you might include  penalties for end gaps, i.e
        # alignment_matrix[i-1, N] = alignment_matrix[i, N] + gap_open
        D_matrix[i-1, N] = 0
        P_matrix[i-1, N] = 0
        Q_matrix[i-1, N] = 0
        E_matrix[i-1, N] = 0

    for j in range(N, 0, -1):
        # Here you might include  penalties for end gaps, i.e
        #alignment_matrix[M, j-1] = alignment_matrix[M, j] + gap_open
        D_matrix[M, j-1] = 0
        P_matrix[M, j-1] = 0
        Q_matrix[M, j-1] = 0
        E_matrix[M, j-1] = 0


    # Main loop
    D_matrix_max_score, D_matrix_i_max, D_matrix_j_max = -9, -9, -9
    for i in range(M-1, -1, -1):
        for j in range(N-1, -1, -1):

            # Q_matrix[i,j] entry
            gap_open_database = D_matrix[i+1,j] + gap_open
            gap_extension_database = Q_matrix[i+1,j] + gap_extension
            max_gap_database = max(gap_open_database, gap_extension_database)

            Q_matrix[i,j] = max_gap_database

            # P_matrix[i,j] entry
            gap_open_query = D_matrix[i,j+1] + gap_open
            gap_extension_query = P_matrix[i,j+1] + gap_extension
            max_gap_query = max(gap_open_query, gap_extension_query)

            P_matrix[i,j] = max_gap_query

            # D_matrix[i,j] entry
            diagonal_score = D_matrix[i+1,j+1] + scoring_scheme[query[i]][database[j]]

            # E_matrix[i,j] entry
            candidates = [(1, diagonal_score),
                          (2, gap_open_database),
                          (4, gap_open_query),
                          (3, gap_extension_database),
                          (5, gap_extension_query)]

            direction, max_score = max(candidates, key=lambda x: x[1])

            # check entry sign
            if max_score > 0:
                E_matrix[i,j] = direction
                D_matrix[i, j] = max_score
            else:
                E_matrix[i,j] = 0
                D_matrix[i, j] = 0

            # fetch global max score
            if max_score > D_matrix_max_score:
                D_matrix_max_score = max_score
                D_matrix_i_max = i
                D_matrix_j_max = j

    return P_matrix, Q_matrix, D_matrix, E_matrix, D_matrix_i_max, D_matrix_j_max, D_matrix_max_score


def pearsons_cc(y_est, y_true):
    """ Calucates the Pearson's correlation coefficient.
    Author: Mathias Rahbek-Borre, Fall 2020.
    """
    n = len(y_est)
    sum_x = sum(y_est)
    sum_y = sum(y_true)
    sum_xy = sum([x*y for x,y in zip(y_est,y_true)])
    sum_xx = sum([x*x for x in y_est])
    sum_yy = sum([y*y for y in y_true])

    r_xy = (n*sum_xy - sum_x*sum_y)/(math.sqrt(n*sum_xx-sum_x**2)*math.sqrt(n*sum_yy-sum_y**2))
    return r_xy


# ## Alignment Matrix Traceback

# ### Traceback

# In[12]:


def smith_waterman_traceback(E_matrix, D_matrix, i_max, j_max, query="VLLP", database="VLILP", gap_open=-5, gap_extension=-1):

    M = len(query)
    N = len(database)

    aligned_query = []
    aligned_database = []

    # start from max_i, max_j
    i, j = i_max, j_max
    matches = 0
    while i < M and j < N:

        # E[i,j] = 0, stop back tracking
        if E_matrix[i, j] == 0:
            break


        # E[i,j] = 1, match
        if E_matrix[i, j] == 1:
            aligned_query.append(query[i])
            aligned_database.append(database[j])
            if ( query[i] == database[j]):
                matches += 1
            i += 1
            j += 1


        # E[i,j] = 2, gap opening in database
        if E_matrix[i, j] == 2:
            aligned_database.append("-")
            aligned_query.append(query[i])
            i += 1


        # E[i,j] = 3, gap extension in database
        if E_matrix[i, j] == 3:

            count = i + 2
            score = D_matrix[count, j] + gap_open + gap_extension

            # Find length of gap
            while((score - D_matrix[i, j])*(score - D_matrix[i, j]) >= 0.00001):
                count += 1
                score = D_matrix[count, j] + gap_open + (count-i-1)*gap_extension

            for k in range(i, count):
                aligned_database.append("-")
                aligned_query.append(query[i])
                i += 1


        # E[i,j] = 4, gap opening in query
        if E_matrix[i, j] == 4:
            aligned_query.append("-")
            aligned_database.append(database[j])
            j += 1


        # E[i,j] = 5, gap extension in query
        if E_matrix[i, j] == 5:

            count = j + 2
            score = D_matrix[i, count] + gap_open + gap_extension

            # Find length of gap
            while((score - D_matrix[i, j])*(score - D_matrix[i, j]) >= 0.0001):
                count += 1
                score = D_matrix[i, count] + gap_open + (count-j-1)*gap_extension

            for k in range(j, count):
                aligned_query.append("-")
                aligned_database.append(database[j])
                j += 1


    return aligned_query, aligned_database, matches


def needleman_wunsch(ori, var, match = 1, mismatch = -1, gap = -1):
    """ Aligns two sequences using Needleman-Wunsch alignment.
        Outputs the two aligned sequences, number of matches and a percent
        similarity.

        Author: Mathias Rahbek-Borre, 6-9-2021
        Inspired by: https://gist.github.com/slowkow/06c6dba9180d013dfd82bec217d22eb5
    """

    n_ori = len(ori)
    n_var = len(var)

    D = np.zeros((n_ori+1, n_var+1))
    P = np.zeros((n_ori+1, n_var+1))

    D[:,0] = np.arange(0,-(n_ori+1),-1)
    D[0,:] = np.arange(0,-(n_var+1),-1)
    P[:,0] = 4
    P[0,:] = 3
    P[0,0] = 0

    for i in range(n_ori):
        for j in range(n_var):
            if ori[i] == var[j]:
                diag = D[i,j] + match
            else:
                diag = D[i,j] + mismatch

            up = D[i+1,j] + gap
            left = D[i,j+1] + gap

            max_score = max(diag,up,left)

            D[i+1,j+1] = max_score

            if diag == max_score:
                P[i+1,j+1] = 2
            elif up == max_score:
                P[i+1,j+1] = 3
            elif left == max_score:
                P[i+1,j+1] = 4

    # print(D)
    # print(P)

    # traceback
    ori_align = ""
    var_align = ""

    i = n_ori
    j = n_var

    while i > 0 or j > 0:
        # print(i,j,P[i,j])

        if P[i,j] == 2:
            i -= 1
            j -= 1
            ori_align = ori[i] + ori_align
            var_align = var[j] + var_align
        elif P[i,j] == 3:
            j -= 1
            ori_align = "-" + ori_align
            var_align = var[j] + var_align
        elif P[i,j] == 4:
            i -= 1
            ori_align = ori[i] + ori_align
            var_align = "-" + var_align

    aligned_similarity = 0
    matches = 0
    for j in range(len(ori_align)):
        if ori_align[j] == var_align[j]:
            aligned_similarity += 100/len(ori_align)
            matches += 1

    return ori_align, var_align, aligned_similarity, matches


# def name_peptides(seqs):
#     unique_seqs = set()
#     allergen_dict = dict()
#     pep_list = []
#     for [name, seq] in seqs_for_FASTA:
#         if seq not in unique_seqs:
#             unique_seqs.add(seq)
#
#             if name in allergen_dict:
#                 allergen_dict[name] += 1
#             else:
#                 allergen_dict[name] = 1
#
#             pep_list.append([name + "_" + str(allergen_dict[name]), seq])
#     return pep_list


def heatmap(pep_list, donor_list, donor_reaction_dict):
    # Heatmap generation
    donor_reaction_overview = np.zeros((len(donor_list), len(pep_list)))
    for i in range(len(pep_list)):
        for j in range(len(donor_list)):
            donor_reaction_overview[j,i] = donor_reaction_dict.get(donor_list[j]).get(pep_list[i][1],-1)

    plt.imshow(donor_reaction_overview, interpolation='nearest', vmax=20)
    plt.show()


def print_corr_plot(chart, corr, dest = "Figures/{}.png"):
    fig, ax = plt.subplots()
    ax.scatter(chart[0], chart[1], label="PCC: %.3f" % PCC)
    ax.legend()
    ax.set_xlabel("Ori SI")
    ax.set_ylabel("Var SI")
    ax.set_title(chart[2])
    fig.savefig(dest.format(chart[2].replace("\n", " ")))
    # plt.show()
    plt.close()


def print_stats(bins):
        print("ttest <50% vs. 50%-80%:")
        print(st.ttest_ind(bins[0], bins[1], equal_var=False))
        print("ttest <50% vs. >80%:")
        print(st.ttest_ind(bins[0], bins[2], equal_var=False))
        print("ttest 50%-80% vs. >80%:")
        print(st.ttest_ind(bins[1], bins[2], equal_var=False))


def corr_v_sim_func(cross_react_count, coef_sim_matrix):
    mean_CR = [np.mean(cross_react_count[0]), np.mean(cross_react_count[1]), np.mean(cross_react_count[2])]

    x1, x2, y1, y2 = [],[],[],[]

    for i in range(len(coef_sim_matrix[0])):
        if coef_sim_matrix[0][i] > 0.5:
            y1.append(coef_sim_matrix[0][i])
            x1.append(coef_sim_matrix[2][i])
        else:
            y2.append(coef_sim_matrix[0][i])
            x2.append(coef_sim_matrix[2][i])

    fig, (ax1, ax2) = plt.subplots(2, 1)
    ax1.scatter(x1,y1, c="blue")
    ax1.scatter(x2,y2, c="black")
    ax1.set_ylabel("Pearson corr. coeff.")
    ax2.bar(["<50%", "50%-80%", ">=80%"], mean_CR)
    ax2.set_xlabel("% Sequence identity", labelpad=5)
    ax2.set_ylabel("Fraction significant")
    plt.savefig("Figures/PCC_v_sim.png")
    sim_v_PCC_PCC = pearsons_cc(coef_sim_matrix[2],coef_sim_matrix[0])
    print("PCC for scatterplot.",sim_v_PCC_PCC)
    plt.show()


def pcc_src_comparison(coef_sim_matrix):
    #PCC histogram
    #calculation of optimal number of bins
    q25, q75 = np.percentile(coef_sim_matrix[0],[.25,.75])
    bin_width = 2*(q75 - q25)*len(coef_sim_matrix)**(-1/3)
    bins = round((max(coef_sim_matrix[0]) - min(coef_sim_matrix[0]))/bin_width)

    #Histograms (PCC & SRC)
    fig, (ax1, ax2) = plt.subplots(2, 1)
    ax1.hist(coef_sim_matrix[0], density = False, bins = bins)
    ax1.set_xlabel("PCC value")
    ax1.set_ylabel("Count")
    ax2.hist(coef_sim_matrix[1], density = False, bins = bins)
    ax2.set_xlabel("SRC value")
    ax2.set_ylabel("Count")

    #SCR/PCC scatterplot
    fig, ax1 = plt.subplots()
    ax1.scatter(coef_sim_matrix[0],coef_sim_matrix[1])
    ax1.set_xlabel("PCC")
    ax1.set_ylabel("SRC")
    plt.show()

    #cross-reaction table
    total_peptide_pairs = len(coef_sim_matrix[0])
    n_PCC_CR = sum([pcc > 0.5 for pcc in coef_sim_matrix[0]])
    n_SRC_CR = sum([src > 0.5 for src in coef_sim_matrix[1]])
    print("Peptide pair crossreativity overview:")
    print("Total: {:<8} PCC:{:<12} SRC:{:<12}".format(total_peptide_pairs, n_PCC_CR, n_SRC_CR))


def load_pep_HLA_data(datafile="Data/2860_NetMHCIIpan.xls"):
    infile = open(datafile,"r")

    infile.readline()
    infile.readline()
    pep_HLA_dict = dict()
    old_pep = 0
    for line in infile:
        line = line.split()

        cur_pep = line[2]
        if old_pep != cur_pep:
            if old_pep:
                pep_HLA_dict[old_pep] = [2 if a<1 else 1 if a<5 else 0 for a in pep_HLA_dict[old_pep]]

            old_pep = cur_pep

        HLA_bind_rank = [float(line[i]) for i in range(6,25,3)]

        if cur_pep in pep_HLA_dict:
            pep_HLA_dict[cur_pep] = [min(old, new) for old, new in zip(pep_HLA_dict[cur_pep], HLA_bind_rank)]
        else:
            pep_HLA_dict[cur_pep] = HLA_bind_rank

    pep_HLA_dict[old_pep] = [2 if a<1 else 1 if a<5 else 0 for a in pep_HLA_dict[old_pep]]

    return pep_HLA_dict


## Main
infile = open("Data/ragweed_Tcell_pairwise.MNi.tab", "r")

infile.readline()


# ['5540', 'Amb', 'a', '1.0101', 'NSDKTIDGRGAKVEIINAGF', '3.74433',
#          'Amb', 'a', '1.0201', 'NSDKTIDGRGVKVNIVNAGL', '1.12407']

i = -1
old_ori_seq = ""
old_var_seq = ""
charts = []
wanted_charts = 10000
n = 0
seqs_for_FASTA = []
donor_reaction_dict = dict()
donor_list = []

pep_HLA_dict = load_pep_HLA_data()

unique_seqs = set()
allergen_dict = dict()
pep_list = []

for line in infile:
    line = line.split()

    donor_id = line[0]
    if donor_id not in donor_list:
        donor_list.append(donor_id)

    ori_pepseq = line[4]
    var_pepseq = line[9]

    ori_SI = float(line[5])
    var_SI = float(line[10])

    if ori_pepseq != old_ori_seq or var_pepseq != old_var_seq:
        i += 1
        if i > wanted_charts-1:
            break
        n += 1
        old_ori_seq = ori_pepseq
        old_var_seq = var_pepseq

        ori_name = line[1] + "_" + line[2] + "_" + line[3]
        var_name = line[6] + "_" + line[7] + "_" + line[8]

        ori_id = ori_name + "_" + ori_pepseq
        var_id = var_name + "_" + var_pepseq

        if ori_id not in unique_seqs:
            unique_seqs.add(ori_id)

            if ori_name in allergen_dict:
                allergen_dict[ori_name] += 1
            else:
                allergen_dict[ori_name] = 1

            full_ori_name = ori_name + "_" + str(allergen_dict[ori_name])
            pep_list.append([full_ori_name, ori_pepseq, pep_HLA_dict[full_ori_name]])

        if var_id not in unique_seqs:
            unique_seqs.add(var_id)

            if var_name in allergen_dict:
                allergen_dict[var_name] += 1
            else:
                allergen_dict[var_name] = 1

            full_var_name = var_name + "_" + str(allergen_dict[var_name])
            pep_list.append([full_var_name, var_pepseq, pep_HLA_dict[full_var_name]])

        ## Similarity measurements
        # Global alignment with Needleman-Wunsch

        ori_align, var_align, aligned_similarity, nw_matches = needleman_wunsch(ori_pepseq, var_pepseq)

        # # local alignment? with Smith-Waterman (O2)
        # scoring_scheme = blosum50
        # gap_open = -11
        # gap_extension = -1

        # P_matrix, Q_matrix, D_matrix, E_matrix, i_max, j_max, max_score = smith_waterman_alignment(ori_pepseq, var_pepseq, scoring_scheme, gap_open, gap_extension)
        # aligned_query, aligned_database, matches = smith_waterman_traceback(E_matrix, D_matrix, i_max, j_max, ori_pepseq, var_pepseq, gap_open, gap_extension)

        # print("ALN", "Origin", len(ori_pepseq), "Variant", len(var_pepseq), len(aligned_query), matches, max_score)
        # print("QAL", i_max, ''.join(aligned_query))
        # print("DAL", j_max,''.join(aligned_database))
        # print("")


        charts.append([[],[], [ori_name, var_name], aligned_similarity, 0])

    charts[i][0].append(ori_SI)
    charts[i][1].append(var_SI)
    charts[i][4] += 1

    if donor_id not in donor_reaction_dict:
        donor_reaction_dict[donor_id] = dict()

    donor_reaction_dict[donor_id][ori_pepseq] = ori_SI
    donor_reaction_dict[donor_id][var_pepseq] = var_SI

infile.close()

print(pep_list)

# # Print seqs for fasta format
# outfile = open("seqs_for_HLA_profiling.fsa", "w")
# for [name, seq] in pep_list:
#     print(">" + name, file=outfile)
#     print(seq, file=outfile)
# outfile.close()

# heatmap(pep_list, donor_list, donor_reaction_dict)


coef_sim_matrix = [[],[],[]]
cross_react_count = [[],[],[]]
PCC_bins = [[],[],[]]

for chart in charts:
    PCC = pearsons_cc(chart[0], chart[1])
    SRC, p = spearmanr(chart[0], chart[1])
    percent_sim = chart[3]

    # print_corr_plot(chart, PCC)

    # print("{:<8} {:<12} {:<12} {:<10}".format("n = %.d" % chart[4], "PCC: %.3f" % PCC, "SRC: %.3f" % SRC, "N_sim: %.d " % chart[3]))

    coef_sim_matrix[0].append(PCC)
    coef_sim_matrix[1].append(SRC)
    coef_sim_matrix[2].append(percent_sim)

    CR = 1 if PCC > 0.5 else 0

    if percent_sim < 50:
        cross_react_count[0].append(CR)
        PCC_bins[0].append(PCC)
    elif percent_sim >= 80:
        cross_react_count[2].append(CR)
        PCC_bins[2].append(PCC)
    else:
        cross_react_count[1].append(CR)
        PCC_bins[1].append(PCC)

# print("Crossreaction frequency t-test")
# print_stats(cross_react_count)
#
# print("PCC t-test")
# print_stats(PCC_bins)
#
# print("Correlation vs. similarity + bar plot")
# corr_v_sim_func(cross_react_count, coef_sim_matrix)
#
# print("PCC and SRC histogram")
# pcc_src_comparison(coef_sim_matrix)
